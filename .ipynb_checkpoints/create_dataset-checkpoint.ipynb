{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antibody Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**  \n",
    "Get a list containing PMCID and PMID from ```pmcids-pmids.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resources/pmcids-pmids.txt', 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each lines seperate between PMID and PMCID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pmids_and_pmcids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    sep_line = line.split('\\t')\n",
    "    pmid = sep_line[0]\n",
    "    pmcid = sep_line[1].replace('\\n', '')\n",
    "    \n",
    "    list_of_pmids_and_pmcids.append({ 'pmid': pmid, 'pmcid': pmcid })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**  \n",
    "find the snippets from nxml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "from tqdm import trange\n",
    "import pprint\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ploy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find only ```<p>``` and then extract the sentences we want (find the regex pattern):\n",
    "  \n",
    "All others tags in ```<p>``` I convert them back to string and remove the xml tag out. \n",
    "- (S|s)pecific\n",
    "- (B|b)ackground staining\n",
    "- (C|c)ross( |-)reactiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_xml_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_snippets(text):\n",
    "    \"\"\"\n",
    "    extract snippets from each paragraph\n",
    "    \"\"\"\n",
    "    snippets = []\n",
    "    define_words = ['(S|s)pecific', '((B|b)ackground staining)', '(C|c)ross( |-)reactiv']\n",
    "    # split sentences from text\n",
    "    split_texts = sent_tokenize(text)\n",
    "    for word in define_words:\n",
    "        snippet = []\n",
    "        # find snippet which contains define_words\n",
    "        for s_index in range(len(split_texts)):\n",
    "            word_contain = re.findall(r\"([^.]*?%s[^.]*\\.)\" % word, split_texts[s_index])\n",
    "            if len(word_contain) != 0:\n",
    "                snip = ''\n",
    "                if s_index - 1 >= 0:\n",
    "                    snip = snip + split_texts[s_index-1] + '\\n'\n",
    "                snip = snip + split_texts[s_index] + '\\n'\n",
    "                if s_index + 1 < len(split_texts):\n",
    "                    snip = snip + split_texts[s_index+1] + '\\n'\n",
    "                    \n",
    "                snippet.append(snip)\n",
    "        if len(snippet) != 0:\n",
    "            snippets.append(set(snippet))\n",
    "    if len(snippets) != 0:\n",
    "        return snippets\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_list = []\n",
    "\n",
    "def find_paragraph(node):\n",
    "    \"\"\"\n",
    "    find snippets in <p>\n",
    "    \"\"\"\n",
    "    global snippet_list\n",
    "    if node.tag == 'p':\n",
    "        # convert all contents in <p> to string\n",
    "        xml_str = ElementTree.tostring(node).decode('utf-8')\n",
    "        text = remove_xml_tags(xml_str)\n",
    "\n",
    "        if node.text is not None:\n",
    "            snippets = extract_snippets(text)\n",
    "            if snippets is not None:\n",
    "                snippet_list.append(snippets)\n",
    "    for child in node:\n",
    "        find_paragraph(child)\n",
    "    \n",
    "    return snippet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippets(tree):\n",
    "    \"\"\"\n",
    "    get snippets from each file\n",
    "    \"\"\"\n",
    "    global snippet_list\n",
    "    snippets = []\n",
    "    node = tree.find('./body')\n",
    "\n",
    "    for elem in node:\n",
    "        snippet = find_paragraph(elem)\n",
    "        snippets.extend(snippet)\n",
    "        snippet_list = []\n",
    "        \n",
    "    if snippets is not None and len(snippet) != 0:\n",
    "        return snippets\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources Papers path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_path = 'resources/papers_4chunnan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_snippet(snip):\n",
    "    snip = snip.replace('\\n', ' ')\n",
    "    return snip[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```outputs``` will contains the dict of outputs that we will save in ```.tsv``` file later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse the file, pass an open file handle to parse()  \n",
    "It will read the data, parse the XML, and return an ElementTree object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading and finding snippets in file: 100%|██████████| 2223/2223 [13:44<00:00,  2.70it/s]\n"
     ]
    }
   ],
   "source": [
    "for index in trange(len(list_of_pmids_and_pmcids), desc='reading and finding snippets in file'):\n",
    "    with open(resources_path + list_of_pmids_and_pmcids[index]['pmcid'] + '.nxml', 'rt') as file:\n",
    "        tree = ElementTree.parse(file)\n",
    "        snippets = get_snippets(tree)\n",
    "        if snippets is not None and len(snippets) != 0:\n",
    "            for snips in snippets:\n",
    "                for each_snip in snips:\n",
    "                    for turple in each_snip:\n",
    "                        outputs.append(\n",
    "                            { \n",
    "                              'pmid': list_of_pmids_and_pmcids[index]['pmid'], \n",
    "                              'pmcid': list_of_pmids_and_pmcids[index]['pmcid'], \n",
    "                              'snippet': clean_snippet(turple)\n",
    "                            }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11342"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**  \n",
    "Write outputs to file ```.tsv```  \n",
    "The pattern is ```PMID\\tPMCID\\tSnippet\\tAntibody related?\\tSpecificity?\\n```    \n",
    "In which antibody related? and specificity? are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('train_ex_antibody.tsv', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing to file : 100%|██████████| 11342/11342 [00:00<00:00, 327936.61it/s]\n"
     ]
    }
   ],
   "source": [
    "for article_index in trange(len(outputs), desc='writing to file '):\n",
    "    file.write('%s\\t%s\\t%s\\t\\t\\t\\n' % (outputs[article_index]['pmid'], \n",
    "                                       outputs[article_index]['pmcid'], \n",
    "                                       outputs[article_index]['snippet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
